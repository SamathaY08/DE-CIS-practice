{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\n\n# Create SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"d2506483-0c01-4406-9680-d3f6ded95d69","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["data=[[\"1\"]]\ndf=spark.createDataFrame(data,[\"id\"])\ndf.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"bde95153-dfdc-44e5-8ec9-2fde0c591b94","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+\n| id|\n+---+\n|  1|\n+---+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import *\n\n#current_date() & current_timestamp()\ndf.withColumn(\"current_date\",current_date()) \\\n  .withColumn(\"current_timestamp\",current_timestamp()) \\\n  .show(truncate=False)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"7f2375ce-45f3-452f-a81f-70074e2d0b09","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+------------+-----------------------+\n|id |current_date|current_timestamp      |\n+---+------------+-----------------------+\n|1  |2023-06-13  |2023-06-13 04:09:45.283|\n+---+------------+-----------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#SQL\nspark.sql(\"select current_date(), current_timestamp()\") \\\n     .show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"3ddc8c9e-5889-4a13-a1d2-25b56c499a7a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+--------------+-----------------------+\n|current_date()|current_timestamp()    |\n+--------------+-----------------------+\n|2023-06-13    |2023-06-13 04:10:54.418|\n+--------------+-----------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["\n# Date & Timestamp into custom format\ndf.withColumn(\"date_format\",date_format(current_date(),\"yyyy-MM-dd\")) \\\n  .withColumn(\"to_timestamp\",to_timestamp(current_timestamp(),\"MM-dd-yyyy HH mm ss\")) \\\n  .show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"455e2fb3-75a9-48c6-a59f-831be26ac156","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---+-----------+-----------------------+\n|id |date_format|to_timestamp           |\n+---+-----------+-----------------------+\n|1  |2023-06-13 |2023-06-13 04:12:54.661|\n+---+-----------+-----------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["\n#SQL\nspark.sql(\"select date_format(current_date(),'MM-dd-yyyy') as date_format ,\" + \\\n          \"to_timestamp(current_timestamp(),'MM-dd-yyyy HH mm ss SSS') as to_timestamp\") \\\n     .show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"74431c5f-1e2c-43e5-b3f5-2340dfa7c63f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----------+-----------------------+\n|date_format|to_timestamp           |\n+-----------+-----------------------+\n|06-13-2023 |2023-06-13 04:10:44.548|\n+-----------+-----------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n\n# Create SparkSession\nspark = SparkSession.builder \\\n               .appName('SparkByExamples.com') \\\n               .getOrCreate()\ndata=[[\"1\"]]\ndf=spark.createDataFrame(data,[\"id\"])\n\nfrom pyspark.sql.functions import *\n\n#current_date() & current_timestamp()\ndf.withColumn(\"current_date\",current_date()) \\\n  .withColumn(\"current_timestamp\",current_timestamp()) \\\n  .show(truncate=False)\n\n#SQL\nspark.sql(\"select current_date(), current_timestamp()\") \\\n     .show(truncate=False)\n\n# Date & Timestamp into custom format\ndf.withColumn(\"date_format\",date_format(current_date(),\"MM-dd-yyyy\")) \\\n  .withColumn(\"to_timestamp\",to_timestamp(current_timestamp(),\"MM-dd-yyyy HH mm ss SSS\")) \\\n  .show(truncate=False)\n\n#SQL\nspark.sql(\"select date_format(current_date(),'MM-dd-yyyy') as date_format ,\" + \\\n          \"to_timestamp(current_timestamp(),'MM-dd-yyyy HH mm ss SSS') as to_timestamp\") \\\n     .show(truncate=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"877533f3-17f7-4278-8e1e-b8f0b619b285","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"pyspark-current-date-timestamp","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
