{"cells":[{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\ndataDictionary = [\n        ('James',{'hair':'black','eye':'brown'}),\n        ('Michael',{'hair':'brown','eye':None}),\n        ('Robert',{'hair':'red','eye':'black'}),\n        ('Washington',{'hair':'grey','eye':'grey'}),\n        ('Jefferson',{'hair':'brown','eye':''})\n        ]\n\ndf = spark.createDataFrame(data=dataDictionary, schema = ['name','properties'])\ndf.printSchema()\ndf.show(truncate=False)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"64abc5b3-b505-4db7-ac7d-5c46b29a1b0c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- name: string (nullable = true)\n |-- properties: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\n+----------+-----------------------------+\n|name      |properties                   |\n+----------+-----------------------------+\n|James     |{eye -> brown, hair -> black}|\n|Michael   |{eye -> null, hair -> brown} |\n|Robert    |{eye -> black, hair -> red}  |\n|Washington|{eye -> grey, hair -> grey}  |\n|Jefferson |{eye -> , hair -> brown}     |\n+----------+-----------------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df3=df.rdd.map(lambda x: \\\n    (x.name,x.properties[\"hair\"],x.properties[\"eye\"])) \\\n    .toDF([\"name\",\"hair\",\"eye\"])\ndf3.printSchema()\ndf3.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"38bee894-1c7c-4672-850f-e719c0ca046d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["root\n |-- name: string (nullable = true)\n |-- hair: string (nullable = true)\n |-- eye: string (nullable = true)\n\n+----------+-----+-----+\n|      name| hair|  eye|\n+----------+-----+-----+\n|     James|black|brown|\n|   Michael|brown| null|\n|    Robert|  red|black|\n|Washington| grey| grey|\n| Jefferson|brown|     |\n+----------+-----+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["\ndf.withColumn(\"hair\",df.properties.getItem(\"hair\")) \\\n  .withColumn(\"eye\",df.properties.getItem(\"eye\")) \\\n  .drop(\"properties\") \\\n  .show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"6173b7f0-58ea-4d93-a0f4-6f523bd8d7da","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----------+-----+-----+\n|      name| hair|  eye|\n+----------+-----+-----+\n|     James|black|brown|\n|   Michael|brown| null|\n|    Robert|  red|black|\n|Washington| grey| grey|\n| Jefferson|brown|     |\n+----------+-----+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.withColumn(\"hair\",df.properties[\"hair\"]) \\\n  .withColumn(\"eye\",df.properties.getItem(\"eye\")) \\\n  .drop(\"properties\") \\\n  .show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"0cf51cff-2c79-4981-b280-6a0992d21b1c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----------+-----+-----+\n|      name| hair|  eye|\n+----------+-----+-----+\n|     James|black|brown|\n|   Michael|brown| null|\n|    Robert|  red|black|\n|Washington| grey| grey|\n| Jefferson|brown|     |\n+----------+-----+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.select(explode(map_keys(df.properties))).distinct().show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"fa47d962-0c29-4639-8913-5c772aa819b7","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----+\n| col|\n+----+\n| eye|\n|hair|\n+----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import explode,map_values,col\n\ndf.select(explode(map_values(df.properties))).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"517eda31-7597-4621-b820-45e1b08c3276","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----+\n|  col|\n+-----+\n|brown|\n|black|\n| null|\n|brown|\n|black|\n|  red|\n| grey|\n| grey|\n|     |\n|brown|\n+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import explode,map_keys,col\nkeysDF = df.select(explode(map_keys(df.properties))).distinct()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"fc105d9b-b0a0-4094-a8fc-92b15f89a4cb","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["keysList = keysDF.rdd.map(lambda x:x[0]).collect()\nprint(keysList)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"86af33da-619b-4508-b5ee-49701731e53d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["['eye', 'hair']\n"]}],"execution_count":0},{"cell_type":"code","source":["keyCols = list(map(lambda x: col(\"properties\").getItem(x).alias(str(x)), keysList))\nprint(keyCols)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"79256b4e-8154-4a9e-a81d-5ad03283d53c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["[Column<'properties[eye] AS eye'>, Column<'properties[hair] AS hair'>]\n"]}],"execution_count":0},{"cell_type":"code","source":["#functions\nfrom pyspark.sql.functions import explode,map_keys,col\nkeysDF = df.select(explode(map_keys(df.properties))).distinct()\nkeysList = keysDF.rdd.map(lambda x:x[0]).collect()\nkeyCols = list(map(lambda x: col(\"properties\").getItem(x).alias(str(x)), keysList))\ndf.select(df.name, *keyCols).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"a29cb1c2-7440-4df5-a823-9de45ac2a847","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----------+-----+-----+\n|      name|  eye| hair|\n+----------+-----+-----+\n|     James|brown|black|\n|   Michael| null|brown|\n|    Robert|black|  red|\n|Washington| grey| grey|\n| Jefferson|     |brown|\n+----------+-----+-----+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import SparkSession\nspark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n\ndataDictionary = [\n        ('James',{'hair':'black','eye':'brown'}),\n        ('Michael',{'hair':'brown','eye':None}),\n        ('Robert',{'hair':'red','eye':'black'}),\n        ('Washington',{'hair':'grey','eye':'grey'}),\n        ('Jefferson',{'hair':'brown','eye':''})\n        ]\n\ndf = spark.createDataFrame(data=dataDictionary, schema = ['name','properties'])\ndf.printSchema()\ndf.show(truncate=False)\n\ndf3=df.rdd.map(lambda x: \\\n    (x.name,x.properties[\"hair\"],x.properties[\"eye\"])) \\\n    .toDF([\"name\",\"hair\",\"eye\"])\ndf3.printSchema()\ndf3.show()\n\ndf.withColumn(\"hair\",df.properties.getItem(\"hair\")) \\\n  .withColumn(\"eye\",df.properties.getItem(\"eye\")) \\\n  .drop(\"properties\") \\\n  .show()\n\ndf.withColumn(\"hair\",df.properties[\"hair\"]) \\\n  .withColumn(\"eye\",df.properties[\"eye\"]) \\\n  .drop(\"properties\") \\\n  .show()\n\n# Functions\nfrom pyspark.sql.functions import explode,map_keys,col\nkeysDF = df.select(explode(map_keys(df.properties))).distinct()\nkeysList = keysDF.rdd.map(lambda x:x[0]).collect()\nkeyCols = list(map(lambda x: col(\"properties\").getItem(x).alias(str(x)), keysList))\ndf.select(df.name, *keyCols).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ac965675-f431-47bc-991a-85609880b2fa","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"convert map to columns","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
